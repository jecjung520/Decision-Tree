{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Modules\n",
    "\n",
    "### Pandas\n",
    "Tabular data processing module\n",
    "\n",
    "### Math\n",
    "Mathematical Calculations\n",
    "\n",
    "### Numpy\n",
    "Basic Computing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Superparameter Definition Section\n",
    "- Choose either **regression tree** or **classification tree**\n",
    "- Dataset address\n",
    "- Get feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"Regression\"  #  Algorithm: Classification, Regression\n",
    "algorithm = \"Classification\"  #  Algorithm: Classification, Regression\n",
    "\n",
    "# Dataset1: Text features and text labels\n",
    "#df = pd.read_csv(\"D:/Code/Decision Treee/candidate/decision-trees-for-ml-master/decision-trees-for-ml-master/dataset/golf.txt\")\n",
    "\n",
    "# Dataset2: Mix features and Numeric labels, here you have to change the path to yours.\n",
    "df = pd.read_csv(\"ML-Dataset/golf4.txt\")\n",
    "\n",
    "# This dictionary is used to store feature types of continuous numeric features and discrete literal features for subsequent judgment\n",
    "dataset_features = dict()\n",
    "\n",
    "num_of_columns = df.shape[1]-1\n",
    "#The data type of each column of the data is saved for displaying the data name\n",
    "for i in range(0, num_of_columns):\n",
    "    #Gets the column name and holds the characteristics of a column of data by column\n",
    "    column_name = df.columns[i]\n",
    "    #Save the type of the data\n",
    "    dataset_features[column_name] = df[column_name].dtypes\n",
    "# The size of the indent when display\n",
    "root = 1\n",
    "\n",
    "# If the algorithm selects a regression tree but the label is not a continuous value, an error is reported\n",
    "if algorithm == 'Regression':\n",
    "    if df['Decision'].dtypes == 'object':\n",
    "        raise ValueError('dataset wrong')\n",
    "# If the tag value is continuous, the regression tree must be used\n",
    "if df['Decision'].dtypes != 'object':\n",
    "    algorithm = 'Regression'\n",
    "    global_stdev = df['Decision'].std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Functions\n",
    "### 3.1 ProcessContinuousFeatures\n",
    "Converts continuous digital features to category features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to handle numeric characteristics\n",
    "def processContinuousFeatures(cdf, column_name, entropy):\n",
    "    # Numerical features are arranged in order\n",
    "    unique_values = sorted(cdf[column_name].unique())\n",
    "\n",
    "    subset_ginis = [];\n",
    "    subset_red_stdevs = []\n",
    "\n",
    "    for i in range(0, len(unique_values) - 1):\n",
    "        threshold = unique_values[i]\n",
    "        # Find the segmentation result if the first number is used as the threshold\n",
    "        subset1 = cdf[cdf[column_name] <= threshold]\n",
    "        subset2 = cdf[cdf[column_name] > threshold]\n",
    "        # Calculate the proportion occupied by dividing the two parts\n",
    "        subset1_rows = subset1.shape[0];\n",
    "        subset2_rows = subset2.shape[0]\n",
    "        total_instances = cdf.shape[0]\n",
    "        # In the text feature part, entropy is calculated by using the cycle, \n",
    "        # and in the numeric part, entropy is calculated by using the two groups after segmentation, \n",
    "        # and the degree of entropy reduction is obtained\n",
    "        if algorithm == 'Classification':\n",
    "            decision_for_subset1 = subset1['Decision'].value_counts().tolist()\n",
    "            decision_for_subset2 = subset2['Decision'].value_counts().tolist()\n",
    "\n",
    "            gini_subset1 = 1;\n",
    "            gini_subset2 = 1\n",
    "\n",
    "            for j in range(0, len(decision_for_subset1)):\n",
    "                gini_subset1 = gini_subset1 - math.pow((decision_for_subset1[j] / subset1_rows), 2)\n",
    "\n",
    "            for j in range(0, len(decision_for_subset2)):\n",
    "                gini_subset2 = gini_subset2 - math.pow((decision_for_subset2[j] / subset2_rows), 2)\n",
    "\n",
    "            gini = (subset1_rows / total_instances) * gini_subset1 + (subset2_rows / total_instances) * gini_subset2\n",
    "\n",
    "            subset_ginis.append(gini)\n",
    "\n",
    "        # Take standard deviation as the judgment basis, calculate the decrease value of standard deviation at this time\n",
    "        elif algorithm == 'Regression':\n",
    "            superset_stdev = cdf['Decision'].std(ddof=0)\n",
    "            subset1_stdev = subset1['Decision'].std(ddof=0)\n",
    "            subset2_stdev = subset2['Decision'].std(ddof=0)\n",
    "\n",
    "            threshold_weighted_stdev = (subset1_rows / total_instances) * subset1_stdev + (\n",
    "                        subset2_rows / total_instances) * subset2_stdev\n",
    "            threshold_reducted_stdev = superset_stdev - threshold_weighted_stdev\n",
    "            subset_red_stdevs.append(threshold_reducted_stdev)\n",
    "\n",
    "    #Find the index of the split value\n",
    "    if algorithm == \"Classification\":\n",
    "        winner_one = subset_ginis.index(min(subset_ginis))\n",
    "    elif algorithm == \"Regression\":\n",
    "        winner_one = subset_red_stdevs.index(max(subset_red_stdevs))\n",
    "    # Find the corresponding value according to the index\n",
    "    winner_threshold = unique_values[winner_one]\n",
    "\n",
    "    # Converts the original data column to an edited string column. \n",
    "    # Characters smaller than the threshold are modified with the <= threshold value\n",
    "    cdf[column_name] = np.where(cdf[column_name] <= winner_threshold, \"<=\" + str(winner_threshold),\">\" + str(winner_threshold))\n",
    "\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CalculateEntropy\n",
    "Calculates Gini or variances for classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the entropy of the column, and the input data must contain the Decision column\n",
    "def calculateEntropy(df):\n",
    "    # The regression tree entropy is 0\n",
    "    if algorithm == 'Regression':\n",
    "        return 0\n",
    "    \n",
    "    rows = df.shape[0]\n",
    "    # Use Value_counts to get all values stored as dictionaries, keys: finds keys, and Tolist: change to  lists.\n",
    "    # This line of code finds the tag value.\n",
    "    decisions = df['Decision'].value_counts().keys().tolist()\n",
    "\n",
    "    entropy = 0\n",
    "    # Here the loop traverses all the tags\n",
    "    for i in range(0, len(decisions)):\n",
    "        # Record the number of times the tag value appears\n",
    "        num_of_decisions = df['Decision'].value_counts().tolist()[i]\n",
    "        # probability of occurrence\n",
    "        class_probability = num_of_decisions / rows\n",
    "        # Calculate the entropy and sum it up\n",
    "        entropy = entropy - class_probability * math.log(class_probability, 2)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 FindDecision\n",
    "Find which features to classify in current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main purpose of this function is to traverse the entire column of the table, \n",
    "# find which column is the best split column, and return the name of the column\n",
    "def findDecision(ddf):\n",
    "    # If it's a regression tree, then you take the standard deviation of the true value\n",
    "    if algorithm == 'Regression':\n",
    "        stdev = ddf['Decision'].std(ddof=0)\n",
    "    # Get the entropy of the decision column\n",
    "    entropy = calculateEntropy(ddf)\n",
    "\n",
    "    columns = ddf.shape[1];\n",
    "    rows = ddf.shape[0]\n",
    "    # Used to store Gini and standard deviation values\n",
    "    ginis = [];\n",
    "    reducted_stdevs = []\n",
    "    # Traverse all columns and calculate the relevant indexes of all columns according to algorithm selection\n",
    "    for i in range(0, columns - 1):\n",
    "        column_name = ddf.columns[i]\n",
    "        column_type = ddf[column_name].dtypes\n",
    "\n",
    "        # Determine if the column feature is a number, and if so, process the data using the following function, \n",
    "        # which modifies the data to a string type category on return.\n",
    "        # The idea is to directly use character characteristics, continuous digital characteristics into discrete character characteristics\n",
    "        if column_type != 'object':\n",
    "            ddf = processContinuousFeatures(ddf, column_name, entropy)\n",
    "        # The statistical data in this column can be obtained, and the continuous data can be directly classified after processing, \n",
    "        # and the categories are less than the threshold and greater than the threshold\n",
    "        classes = ddf[column_name].value_counts()\n",
    "        gini = 0;\n",
    "        weighted_stdev = 0\n",
    "        # Start the loop with the type of data in the column\n",
    "        for j in range(0, len(classes)):\n",
    "            current_class = classes.keys().tolist()[j]\n",
    "            # The final classification result corresponding to the data is selected \n",
    "            # by deleting the value of the df column equal to the current data\n",
    "            subdataset = ddf[ddf[column_name] == current_class]\n",
    "\n",
    "            subset_instances = subdataset.shape[0]\n",
    "            # The entropy of information is calculated here\n",
    "            if algorithm == 'Classification':  # GINI index\n",
    "                decision_list = subdataset['Decision'].value_counts().tolist()\n",
    "\n",
    "                subgini = 1\n",
    "\n",
    "                for k in range(0, len(decision_list)):\n",
    "                    subgini = subgini - math.pow((decision_list[k] / subset_instances), 2)\n",
    "\n",
    "                gini = gini + (subset_instances / rows) * subgini\n",
    "            # The regression tree is judged by the standard deviation, \n",
    "            # and the standard deviation of the subclasses in this column is calculated here\n",
    "            elif algorithm == 'Regression':\n",
    "                subset_stdev = subdataset['Decision'].std(ddof=0)\n",
    "                weighted_stdev = weighted_stdev + (subset_instances / rows) * subset_stdev\n",
    "\n",
    "        # Used to store the final value of this column\n",
    "        if algorithm == \"Classification\":\n",
    "            ginis.append(gini)\n",
    "        # Store the decrease in standard deviation for all columns\n",
    "        elif algorithm == 'Regression':\n",
    "            reducted_stdev = stdev - weighted_stdev\n",
    "            reducted_stdevs.append(reducted_stdev)\n",
    "\n",
    "    # Determine which column is the first branch \n",
    "    # by selecting the index of the largest value from the list of evaluation indicators\n",
    "    if algorithm == \"Classification\":\n",
    "        winner_index = ginis.index(min(ginis))\n",
    "    elif algorithm == \"Regression\":\n",
    "        winner_index = reducted_stdevs.index(max(reducted_stdevs))\n",
    "    winner_name = ddf.columns[winner_index]\n",
    "\n",
    "    return winner_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 FormatRule\n",
    "Standardize final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT is a number used to generate ' 'to adjust the display format of the decision making process\n",
    "def formatRule(root):\n",
    "    resp = ''\n",
    "\n",
    "    for i in range(0, root):\n",
    "        resp = resp + '   '\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 BuildDecisionTree\n",
    "Main function for building decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function, you build the decision tree model, \n",
    "# entering data in dataframe format, the root value, and the file address\n",
    "# If the value in the column is literal, it branches directly by literal category\n",
    "def buildDecisionTree(df, root):\n",
    "    # Identify the different charForResp\n",
    "    charForResp = \"'\"\n",
    "    if algorithm == 'Regression':\n",
    "        charForResp = \"\"\n",
    "\n",
    "    tmp_root = root * 1\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    # Output the winning column of the decision tree, enter a list, \n",
    "    # and output the column name of the decision column in the list\n",
    "    winner_name = findDecision(df)\n",
    "\n",
    "    # Determines whether the winning column is a number or a character\n",
    "    numericColumn = False\n",
    "    if dataset_features[winner_name] != 'object':\n",
    "        numericColumn = True\n",
    "\n",
    "    # To ensure the integrity of the original data and prevent the data from changing, \n",
    "    # mainly to ensure that the data of other columns besides the winning column does not change, \n",
    "    # so as to continue the branch in the next step.\n",
    "    columns = df.shape[1]\n",
    "    for i in range(0, columns - 1):\n",
    "        column_name = df.columns[i]\n",
    "        if df[column_name].dtype != 'object' and column_name != winner_name:\n",
    "            df[column_name] = df_copy[column_name]\n",
    "    # Find the element in the branching column\n",
    "    classes = df[winner_name].value_counts().keys().tolist()\n",
    "    # Traversing all classes in the branch column has two functions: \n",
    "    # 1. Display which class is currently traversed to; 2. Determine whether the current class is already leaf node\n",
    "    for i in range(0, len(classes)):\n",
    "        # Find the Subdataset as in FindDecision, but discard this column of the current branch\n",
    "        current_class = classes[i]\n",
    "        subdataset = df[df[winner_name] == current_class]\n",
    "        # At the same time, the data of the first branch column is discarded and the remaining data is processed\n",
    "        subdataset = subdataset.drop(columns=[winner_name])\n",
    "        # Edit the display situation. If it is a numeric feature, the character conversion has been completed when searching for branches. \n",
    "        #If it is not a character feature, it is displayed with column names\n",
    "        if numericColumn == True:\n",
    "            compareTo = current_class  # current class might be <=x or >x in this case\n",
    "        else:\n",
    "            compareTo = \" == '\" + str(current_class) + \"'\"\n",
    "\n",
    "        terminateBuilding = False\n",
    "\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        # This determines whether it is already the last leaf node\n",
    "        if len(subdataset['Decision'].value_counts().tolist()) == 1:\n",
    "            final_decision = subdataset['Decision'].value_counts().keys().tolist()[\n",
    "                0]  # all items are equal in this case\n",
    "            terminateBuilding = True\n",
    "        # At this time, only the Decision column is left, that is, all the segmentation features have been used\n",
    "        elif subdataset.shape[1] == 1:\n",
    "            # get the most frequent one\n",
    "            final_decision = subdataset['Decision'].value_counts().idxmax()  \n",
    "            terminateBuilding = True\n",
    "        # The regression tree is judged as leaf node if the number of elements is less than 5\n",
    "        #elif algorithm == 'Regression' and subdataset.shape[0] < 5:  # pruning condition\n",
    "        # Another criterion is to take the standard deviation as the criterion and the sample mean in the node as the value of the node\n",
    "        elif algorithm == 'Regression' and subdataset['Decision'].std(ddof=0)/global_stdev < 0.4:\n",
    "            # get average\n",
    "            final_decision = subdataset['Decision'].mean()  \n",
    "            terminateBuilding = True\n",
    "        # -----------------------------------------------\n",
    "        # Here we begin to output the branching results of the decision tree.ã€‚\n",
    "\n",
    "        print(formatRule(root), \"if \", winner_name, compareTo, \":\")\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # check decision is made\n",
    "        if terminateBuilding == True:  \n",
    "            print(formatRule(root + 1), \"return \", charForResp + str(final_decision) + charForResp)\n",
    "        else:  # decision is not made, continue to create branch and leafs\n",
    "            # The size of the indent at display represented by root\n",
    "            root = root + 1\n",
    "            # Call this function again for the next loop\n",
    "            buildDecisionTree(subdataset, root)\n",
    "\n",
    "        root = tmp_root * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    if  Outlook  == 'Rain' :\n",
      "       if  Wind  == 'Weak' :\n",
      "          return  48.0\n",
      "       if  Wind  == 'Strong' :\n",
      "          return  27.5\n",
      "    if  Outlook  == 'Sunny' :\n",
      "       if  Temp. <=83 :\n",
      "          if  Wind  == 'Strong' :\n",
      "             if  Humidity <=95 :\n",
      "                return  30\n",
      "          if  Wind  == 'Weak' :\n",
      "             return  36.0\n",
      "       if  Temp. >83 :\n",
      "          return  25\n",
      "    if  Outlook  == 'Overcast' :\n",
      "       return  45.75\n"
     ]
    }
   ],
   "source": [
    "buildDecisionTree(df, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
